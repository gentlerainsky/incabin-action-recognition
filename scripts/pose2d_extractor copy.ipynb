{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from modules.pose_extractor.pose2d_estimator.pose2d_estimator import Pose2DEstimator\n",
    "from modules.pose_extractor.pose2d_estimator.pose2d_extractor import Pose2DExtractor\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_root = Path('/root/data/raw/drive_and_act/')\n",
    "# video_path = video_root / 'inner_mirror' / 'vp1' / 'run1b_2018-05-29-14-02-47.ids_1.mp4'\n",
    "annotation_path = video_root / 'iccv_activities_3s' / 'activities_3s' / 'inner_mirror' / 'midlevel.chunks_90.csv'\n",
    "pose_estimator_2d = Pose2DEstimator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split 0\n",
    "# train 1,2,3,4,6,7,8,9,10,12\n",
    "# val 14, 15\n",
    "# test 5, 11, 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = Pose2DExtractor(\n",
    "    video_root_path=video_root.as_posix(),\n",
    "    data_subset='train',\n",
    "    pose_estimator_2d=pose_estimator_2d,\n",
    "    pickle_output_path='./output/inner_mirror_with_padding'\n",
    ")\n",
    "extractor.extract_2d_pose_from_annotation_file(annotation_path.as_posix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df = pd.read_csv(str(annotation_path))\n",
    "# # with pd.option_context('display.max_rows', None, 'display.max_columns', None):  # more options can be specified also\n",
    "# #     display(df[df['participant_id'] == 1])\n",
    "# df = df.set_index(['participant_id', 'file_id'])\n",
    "\n",
    "# frame_info = []\n",
    "# pose_2d_results = []\n",
    "# current_video_name = None\n",
    "# num_rows = df.shape[0]\n",
    "# frame_count = 0\n",
    "# annotation_count = 0\n",
    "# for index, row in df.iterrows():\n",
    "#     if (count + 1) % 100 == 0:\n",
    "#         print(f'{((index + 1) / num_rows * 100):.2f}%')\n",
    "#     annotation_count += 1\n",
    "#     participant_id, video_name = index\n",
    "#     if current_video_name != video_name:\n",
    "#         video_file = str(video_path)\n",
    "#         cap = cv2.VideoCapture(video_file)\n",
    "#         total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "#         frame_rate = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "#         # in Hz\n",
    "#         sampling_rate = 15\n",
    "#         sampling_rate = int((1 / sampling_rate) * np.floor(frame_rate))\n",
    "#     images = []\n",
    "#     start = row['frame_start']\n",
    "#     end = row['frame_end']\n",
    "#     for frame_number in range(start, end, sampling_rate):\n",
    "#         cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "#         _, image = cap.read()\n",
    "#         images.append(image)\n",
    "#     bboxes, pose_2d_list, keypoint_2d_scores = pose_estimator_2d.inference(images)\n",
    "#     for idx, image in enumerate(images):\n",
    "#         frame_info.append(dict(\n",
    "#             frame_index=annotation_count,\n",
    "#             participant_id=participant_id,\n",
    "#             video_name=video_name,\n",
    "#             annotation_id=row['annotation_id'],\n",
    "#             frame=start + idx,\n",
    "#             activity=row['activity'],\n",
    "#         ))\n",
    "#         pose_2d_results.append(dict(\n",
    "#             frame_index=annotation_count,\n",
    "#             pose_2d=pose_2d_list[idx],\n",
    "#             pose_2d_score=keypoint_2d_scores[idx],\n",
    "#             pose_2d_avg_score=np.mean(keypoint_2d_scores[idx])\n",
    "#         ))\n",
    "#         frame_count += 1\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pose_2d_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# frame_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# ax.imshow(images[10])\n",
    "# ax.scatter(pose_2d_list[10][:, 0], pose_2d_list[10][:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mmpose.apis import MMPoseInferencer\n",
    "\n",
    "# # img_path = 'tests/data/coco/000000000785.jpg'   # replace this with your own image path\n",
    "\n",
    "# # instantiate the inferencer using the model alias\n",
    "# inferencer = MMPoseInferencer('td-hm_ViTPose-large-simple_8xb64-210e_coco-256x192')\n",
    "\n",
    "# # The MMPoseInferencer API employs a lazy inference approach,\n",
    "# # creating a prediction generator when given input\n",
    "# result_generator = inferencer(images[0], show=True)\n",
    "# result = next(result_generator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
