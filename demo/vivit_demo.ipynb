{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "all_activities = [\n",
    "    'closing_bottle',\n",
    "    'closing_door_inside',\n",
    "    'closing_door_outside',\n",
    "    'closing_laptop',\n",
    "    'drinking',\n",
    "    'eating',\n",
    "    'entering_car',\n",
    "    'exiting_car',\n",
    "    'fastening_seat_belt',\n",
    "    'fetching_an_object',\n",
    "    'interacting_with_phone',\n",
    "    'looking_or_moving_around (e.g. searching)',\n",
    "    'opening_backpack',\n",
    "    'opening_bottle',\n",
    "    'opening_door_inside',\n",
    "    'opening_door_outside',\n",
    "    'opening_laptop',\n",
    "    'placing_an_object',\n",
    "    'preparing_food',\n",
    "    'pressing_automation_button',\n",
    "    'putting_laptop_into_backpack',\n",
    "    'putting_on_jacket',\n",
    "    'putting_on_sunglasses',\n",
    "    'reading_magazine',\n",
    "    'reading_newspaper',\n",
    "    'sitting_still',\n",
    "    'taking_laptop_from_backpack',\n",
    "    'taking_off_jacket',\n",
    "    'taking_off_sunglasses',\n",
    "    'talking_on_phone',\n",
    "    'unfastening_seat_belt',\n",
    "    'using_multimedia_display',\n",
    "    'working_on_laptop',\n",
    "    'writing'\n",
    "]\n",
    "all_activity_mapper = {all_activities[i]: i for i in range(len(all_activities))}\n",
    "\n",
    "\n",
    "class VideoDataset:\n",
    "    def __init__(self, annotation_df, poses, max_len=30) -> None:\n",
    "        self.pose_info = poses\n",
    "        self.annotation_df = annotation_df\n",
    "        self.max_len = max_len\n",
    "        self.all_activities = set([])\n",
    "        self.samples = []\n",
    "        self.shuffle()\n",
    "\n",
    "    def shuffle(self):\n",
    "        self.activities = []\n",
    "        for idx, annotation in self.annotation_df.iterrows():\n",
    "            frame_start, frame_end = annotation.frame_index_start, annotation.frame_index_end\n",
    "            self.samples.append(dict(\n",
    "                frame_start = frame_start,\n",
    "                frame_end = frame_end,\n",
    "                label=all_activity_mapper[annotation.activity],\n",
    "                file_name=annotation.video_name\n",
    "            ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.annotation_df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return dict(\n",
    "            idx=idx,\n",
    "            activity=self.activities[idx],\n",
    "            pose_2d=self.pose_2d_sequences[idx],\n",
    "            pose_3d=self.pose_3d_sequences[idx],\n",
    "            valid_len=self.sequnce_valid_len[idx],\n",
    "            \n",
    "        )\n",
    "        # {\n",
    "        #     'video': <video_tensor>,     # Shape: (C, T, H, W)\n",
    "        #     'audio': <audio_tensor>,     # Shape: (S)\n",
    "        #     'label': <action_label>,     # Integer defining class annotation\n",
    "        #     'video_name': <video_path>,  # Video file path stem\n",
    "        #     'video_index': <video_id>,   # index of video used by sampler\n",
    "        #     'clip_index': <clip_id>      # index of clip sampled within video\n",
    "        # }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('output/inner_mirror/train_pose_info.pkl', 'rb') as f:\n",
    "    train_pose_info_list = pickle.load(f)\n",
    "train_pose_info_list = sorted(train_pose_info_list, key=lambda x: x['index'])\n",
    "\n",
    "with open('output/inner_mirror/train_annotation.pkl', 'rb') as f:\n",
    "    train_annotation = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participant_id</th>\n",
       "      <th>video_name</th>\n",
       "      <th>annotation_id</th>\n",
       "      <th>activity</th>\n",
       "      <th>frame_index_start</th>\n",
       "      <th>frame_index_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>vp1/run1b_2018-05-29-14-02-47.ids_1</td>\n",
       "      <td>1</td>\n",
       "      <td>closing_door_outside</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>vp1/run1b_2018-05-29-14-02-47.ids_1</td>\n",
       "      <td>3</td>\n",
       "      <td>opening_door_outside</td>\n",
       "      <td>25</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>vp1/run1b_2018-05-29-14-02-47.ids_1</td>\n",
       "      <td>4</td>\n",
       "      <td>entering_car</td>\n",
       "      <td>54</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>vp1/run1b_2018-05-29-14-02-47.ids_1</td>\n",
       "      <td>5</td>\n",
       "      <td>closing_door_inside</td>\n",
       "      <td>80</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>vp1/run1b_2018-05-29-14-02-47.ids_1</td>\n",
       "      <td>6</td>\n",
       "      <td>fastening_seat_belt</td>\n",
       "      <td>97</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   participant_id                           video_name  annotation_id  \\\n",
       "0               1  vp1/run1b_2018-05-29-14-02-47.ids_1              1   \n",
       "1               1  vp1/run1b_2018-05-29-14-02-47.ids_1              3   \n",
       "2               1  vp1/run1b_2018-05-29-14-02-47.ids_1              4   \n",
       "3               1  vp1/run1b_2018-05-29-14-02-47.ids_1              5   \n",
       "4               1  vp1/run1b_2018-05-29-14-02-47.ids_1              6   \n",
       "\n",
       "               activity  frame_index_start  frame_index_end  \n",
       "0  closing_door_outside                  0               25  \n",
       "1  opening_door_outside                 25               54  \n",
       "2          entering_car                 54               80  \n",
       "3   closing_door_inside                 80               97  \n",
       "4   fastening_seat_belt                 97              152  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_annotation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classes: ['closing_bottle', 'closing_door_inside', 'closing_door_outside', 'closing_laptop', 'drinking', 'eating', 'entering_car', 'exiting_car', 'fastening_seat_belt', 'fetching_an_object', 'interacting_with_phone', 'looking_or_moving_around (e.g. searching)', 'opening_backpack', 'opening_bottle', 'opening_door_inside', 'opening_door_outside', 'opening_laptop', 'placing_an_object', 'preparing_food', 'pressing_automation_button', 'putting_laptop_into_backpack', 'putting_on_jacket', 'putting_on_sunglasses', 'reading_magazine', 'reading_newspaper', 'sitting_still', 'taking_laptop_from_backpack', 'taking_off_jacket', 'taking_off_sunglasses', 'talking_on_phone', 'unfastening_seat_belt', 'using_multimedia_display', 'working_on_laptop', 'writing'].\n"
     ]
    }
   ],
   "source": [
    "from modules.action_recognizer.dataset.video_dataset import (all_activities, all_activity_mapper)\n",
    "# class_labels = sorted()\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "# id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# print(f\"Unique classes: {list(label2id.keys())}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d94972c789946d8a022a8e318c8ecdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/401 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07330a173a0049608a0695531a1144cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/18.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f48ca412d3943e79bcf80e99c5ebb98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/356M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torch/_utils.py:776: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "Some weights of VivitForVideoClassification were not initialized from the model checkpoint at google/vivit-b-16x2-kinetics400 and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([34, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([34]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import VivitImageProcessor, VivitForVideoClassification\n",
    "\n",
    "# model_ckpt = \"google/vivit-b-16x2-kinetics400\"\n",
    "# image_processor = VivitImageProcessor.from_pretrained(model_ckpt)\n",
    "\n",
    "# model = VivitForVideoClassification.from_pretrained(\n",
    "#     model_ckpt,\n",
    "#     label2id=label2id,\n",
    "#     id2label=id2label,\n",
    "#     ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pytorchvideo.data\n",
    "\n",
    "# from pytorchvideo.transforms import (\n",
    "#     ApplyTransformToKey,\n",
    "#     Normalize,\n",
    "#     RandomShortSideScale,\n",
    "#     RemoveKey,\n",
    "#     ShortSideScale,\n",
    "#     UniformTemporalSubsample,\n",
    "# )\n",
    "\n",
    "# from torchvision.transforms import (\n",
    "#     Compose,\n",
    "#     Lambda,\n",
    "#     RandomCrop,\n",
    "#     RandomHorizontalFlip,\n",
    "#     Resize,\n",
    "# )\n",
    "\n",
    "# mean = image_processor.image_mean\n",
    "# std = image_processor.image_std\n",
    "# if \"shortest_edge\" in image_processor.size:\n",
    "#     height = width = image_processor.size[\"shortest_edge\"]\n",
    "# else:\n",
    "#     height = image_processor.size[\"height\"]\n",
    "#     width = image_processor.size[\"width\"]\n",
    "# resize_to = (height, width)\n",
    "\n",
    "# num_frames_to_sample = model.config.num_frames\n",
    "# sample_rate = 4\n",
    "# fps = 30\n",
    "# clip_duration = num_frames_to_sample * sample_rate / fps\n",
    "\n",
    "# train_transform = Compose(\n",
    "#     [\n",
    "#         ApplyTransformToKey(\n",
    "#             key=\"video\",\n",
    "#             transform=Compose(\n",
    "#                 [\n",
    "#                     UniformTemporalSubsample(num_frames_to_sample),\n",
    "#                     Lambda(lambda x: x / 255.0),\n",
    "#                     Normalize(mean, std),\n",
    "#                     RandomShortSideScale(min_size=256, max_size=320),\n",
    "#                     RandomCrop(resize_to),\n",
    "#                     RandomHorizontalFlip(p=0.5),\n",
    "#                 ]\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# train_dataset = pytorchvideo.data.Ucf101(\n",
    "#     data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "#     clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "#     decode_audio=False,\n",
    "#     transform=train_transform,\n",
    "# )\n",
    "\n",
    "# val_transform = Compose(\n",
    "#     [\n",
    "#         ApplyTransformToKey(\n",
    "#             key=\"video\",\n",
    "#             transform=Compose(\n",
    "#                 [\n",
    "#                     UniformTemporalSubsample(num_frames_to_sample),\n",
    "#                     Lambda(lambda x: x / 255.0),\n",
    "#                     Normalize(mean, std),\n",
    "#                     Resize(resize_to),\n",
    "#                 ]\n",
    "#             ),\n",
    "#         ),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# val_dataset = pytorchvideo.data.Ucf101(\n",
    "#     data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "#     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "#     decode_audio=False,\n",
    "#     transform=val_transform,\n",
    "# )\n",
    "\n",
    "# test_dataset = pytorchvideo.data.Ucf101(\n",
    "#     data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "#     clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "#     decode_audio=False,\n",
    "#     transform=val_transform,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import imageio\n",
    "# import numpy as np\n",
    "# from IPython.display import Image\n",
    "\n",
    "# def unnormalize_img(img):\n",
    "#     \"\"\"Un-normalizes the image pixels.\"\"\"\n",
    "#     img = (img * std) + mean\n",
    "#     img = (img * 255).astype(\"uint8\")\n",
    "#     return img.clip(0, 255)\n",
    "\n",
    "# def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "#     \"\"\"Prepares a GIF from a video tensor.\n",
    "    \n",
    "#     The video tensor is expected to have the following shape:\n",
    "#     (num_frames, num_channels, height, width).\n",
    "#     \"\"\"\n",
    "#     frames = []\n",
    "#     for video_frame in video_tensor:\n",
    "#         frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "#         frames.append(frame_unnormalized)\n",
    "#     kargs = {\"duration\": 0.25}\n",
    "#     imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "#     return filename\n",
    "\n",
    "# def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "#     \"\"\"Prepares and displays a GIF from a video tensor.\"\"\"\n",
    "#     video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "#     gif_filename = create_gif(video_tensor, gif_name)\n",
    "#     return Image(filename=gif_filename)\n",
    "\n",
    "# sample_video = next(iter(train_dataset))\n",
    "# video_tensor = sample_video[\"video\"]\n",
    "# display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/docs/transformers/en/tasks/video_classification\n",
    "https://huggingface.co/docs/transformers/main/model_doc/vivit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
