{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/inner_mirror/train_pose2D.pkl', 'rb') as f:\n",
    "    pose2d_list = pickle.load(f)\n",
    "\n",
    "with open('output/inner_mirror/train_frame_info.pkl', 'rb') as f:\n",
    "    frame_info_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.pose_extractor.pose3d_estimator.pose3d_estimator import Pose3DEstimator\n",
    "from modules.pose_extractor.pose3d_estimator.jointformer.lit_jointformer import LitJointFormer\n",
    "from modules.pose_extractor.pose3d_estimator.pose2d_dataset import Pose2DDataset\n",
    "\n",
    "\n",
    "pose2d_dataset = Pose2DDataset(frame_info_list=frame_info_list, pose2d_list=pose2d_list)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        pose2d_dataset, batch_size=256, num_workers=24, shuffle=True\n",
    "    )\n",
    "\n",
    "pose3d_estimator = Pose3DEstimator(\n",
    "    LitModel=LitJointFormer,\n",
    "    lifter_saved_model='saved_models/pose3d_estimator/all_actors/jointformer/lightning_logs/version_0/checkpoints/epoch=34-step=12285.ckpt'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 981/981 [00:14<00:00, 68.10it/s]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for batch in tqdm(iter(train_loader)):\n",
    "    pose3d = pose3d_estimator.inference(batch)\n",
    "    dict_results = []\n",
    "    for index, pose_2d, root_2d, scale_factor_w, scale_factor_h, pose3d in list(\n",
    "        zip(\n",
    "            batch['frame_index'].detach().cpu().numpy().tolist(),\n",
    "            batch['pose_2d'].detach().cpu().numpy().tolist(),\n",
    "            batch['root_2d'].detach().cpu().numpy().tolist(),\n",
    "            batch['scale_factor'][0].detach().cpu().numpy().tolist(),\n",
    "            batch['scale_factor'][1].detach().cpu().numpy().tolist(),\n",
    "            pose3d.detach().cpu().numpy().tolist()\n",
    "        )\n",
    "    ):\n",
    "        dict_results.append(dict(\n",
    "            index=index,\n",
    "            pose_2d=pose_2d,\n",
    "            root_2d=root_2d,\n",
    "            scale_factor_w=scale_factor_w,\n",
    "            scale_factor_h=scale_factor_h,\n",
    "            pose_3d=pose3d))\n",
    "    results += dict_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output/inner_mirror/train_pose_info.pkl', 'wb') as f:\n",
    "    pickle.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
